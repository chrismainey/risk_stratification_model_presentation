
@misc{carequalitycommissioncqcMonitoringNHSAcute2017,
	title = {Monitoring {NHS} acute hospitals {\textbar} {Care} {Quality} {Commission}},
	language = {Large print},
	author = {{Care Quality Commission}},
	year = {2017},
	keywords = {or service name},
}

@techreport{carequalitycommissioncqcNHSAcuteHospitals2014,
	title = {{NHS} acute hospitals: {Statistical} {Methodology}},
	institution = {{Care Quality Commission (CQC)}},
	author = {{Care Quality Commission}},
	year = {2014},
}

@article{griggNullSteadystateDistribution2008,
	title = {The null steady-state distribution of the {CUSUM} statistic},
	doi = {10/bgvkdx},
	journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
	author = {Grigg, O. and Spiegelhalter, D.},
	year = {2008},
}

@article{griggOverviewRiskadjustedCharts2004,
	title = {An overview of risk-adjusted charts},
	volume = {167},
	issn = {0964-1998 1467-985X},
	doi = {10/c2n3h6},
	abstract = {Summary. The paper provides an overview of risk-adjusted charts, with examples based on two data sets: the first consisting of outcomes following cardiac surgery and patient factors contributing to the Parsonnet score; the second being age–sex-adjusted death-rates per year under a single general practitioner. Charts presented include the cumulative sum (CUSUM), resetting sequential probability ratio test, the sets method and Shewhart chart. Comparisons between the charts are made. Estimation of the process parameter and two-sided charts are also discussed. The CUSUM is found to be the least efficient, under the average run length (ARL) criterion, of the resetting sequential probability ratio test class of charts, but the ARL criterion is thought not to be sensible for comparisons within that class. An empirical comparison of the sets method and CUSUM, for binary data, shows that the sets method is more efficient when the in-control ARL is small and more efficient for a slightly larger range of in-control ARLs when the change in parameter being tested for is larger. The Shewart p-chart is found to be less efficient than the CUSUM even when the change in parameter being tested for is large.},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Grigg, O. and Farewell, V.},
	year = {2004},
	keywords = {Average run length, Cumulative sum, Performance, Resetting sequential probability ratio test, Risk-adjusted control charts, Sets method, Shewhart},
	pages = {523--539},
}

@article{griggUseRiskadjustedCUSUM2003,
	title = {Use of risk-adjusted {CUSUM} and {RSPRT} charts for monitoring in medical contexts},
	volume = {12},
	issn = {0962-2802 (Print) 0962-2802 (Linking)},
	doi = {10/gd87wh},
	abstract = {In this paper we discuss the use of charts derived from the sequential probability ratio test (SPRT): the cumulative sum (CUSUM) chart, RSPRT (resetting SPRT), and FIR (fast initial response) CUSUM. The theoretical development of the methods is described and some considerations one might address when designing a chart, explored, including the approximation of average run lengths (ARLs), the importance of detecting improvements in a process as well as detecting deterioration and estimation of the process parameter following a signal. Two examples are used to demonstrate the practical issues of quality control in the medical setting, the first a running example and the second a fully worked example at the end of the paper. The first example relates to 30-day mortality for patients of a single cardiac surgeon over the period 1994-1998, the second to patient deaths in the practice of a single GP, Harold Shipman. The charts' performances relative to each other are shown to be sensitive to the definition of the 'out of control' state of the process being monitored. In light of this, it is stressed that a suitable means by which to compare charts is chosen in any specific application.},
	number = {2},
	journal = {Statistical Methods in Medical Research},
	author = {Grigg, O. A. and Farewell, V. T. and Spiegelhalter, D. J.},
	month = mar,
	year = {2003},
	keywords = {*Medical Records, Adult, Aged, Female, Humans, Male, Middle Aged, Quality Control, Risk Adjustment/*methods, United Kingdom},
	pages = {147--70},
}

@article{lovegroveMonitoringPerformanceCardiac1999,
	title = {Monitoring the {Performance} of {Cardiac} {Surgeons}},
	volume = {50},
	issn = {01605682, 14769360},
	doi = {10/dtsvbg},
	abstract = {[In the past, simple methods have been used to examine the performance of surgeons by examining the cumulative in-hospital mortality of their patients, without regard to the severity of the cases involved. This paper describes an alternative approach which takes account of an individual cardiac surgeon's case-mix by explicitly incorporating the inherent risk faced by patients due to a combination of factors relating to their age and the degree of disease they have.]},
	number = {7},
	journal = {The Journal of the Operational Research Society},
	author = {Lovegrove, J. and Sherlaw-Johnson, C. and Valencia, O. and Treasure, T. and Gallivan, S.},
	year = {1999},
	note = {Publisher: Palgrave Macmillan Journals},
	pages = {684--689},
}

@article{mohammedPlottingBasicControl2008,
	title = {Plotting basic control charts: {Tutorial} notes for healthcare practitioners},
	volume = {17},
	doi = {10/dkz8wm},
	abstract = {There is considerable interest in the use of statistical process control (SPC) in healthcare. Although SPC is part of an overall philosophy of continual improvement, the implementation of SPC usually requires the production of control charts. However, as SPC is relatively new to healthcare practitioners and is not routinely featured in medical statistics texts/courses, there is a need to explain the issues involved in the selection and construction of control charts in practice. Following a brief overview of SPC in healthcare and preliminary issues, we use a tutorial-based approach to illustrate the selection and construction of four commonly used control charts (xmr-chart, p-chart, u-chart, c-chart) using examples from healthcare. For each control chart, the raw data, the relevant formulae and their use and interpretation of the final SPC chart are provided together with a notes section highlighting important issues for the SPC practitioner. Some more advanced topics are also mentioned with suggestions for further reading.},
	number = {2},
	journal = {Quality and Safety in Health Care},
	author = {Mohammed, M. A. and Worthington, P. and Woodall, W. H.},
	year = {2008},
	pages = {137--145},
}

@article{sherlaw-johnsonMethodDetectingRuns2005,
	title = {A {Method} for {Detecting} {Runs} of {Good} and {Bad} {Clinical} {Outcomes} on {Variable} {Life}-{Adjusted} {Display} ({VLAD}) {Charts}},
	volume = {8},
	issn = {1572-9389},
	doi = {10/dpvrft},
	abstract = {In recent years there has been a growing need for effective monitoring of clinical outcomes. Two techniques for continuous monitoring that have emerged almost simultaneously are the Variable Life-Adjusted Display (VLAD) and risk-adjusted cumulative sum charts (CUSUM). The VLAD provides clinicians and management with an easily understandable overview of outcome history and is now in routine use in several hospitals. Although it can indicate runs of good and bad outcomes, unlike the CUSUM, it does not provide a quantitative means for assessing whether they merit investigation. This paper introduces a scheme for applying control limits from CUSUM charts onto the VLAD, thus enhancing its role as an effective monitoring tool.},
	number = {1},
	journal = {Health Care Management Science},
	author = {Sherlaw-Johnson, Chris},
	month = feb,
	year = {2005},
	pages = {61--65},
}

@article{spiegelhalterFunnelPlotsComparing2005,
	title = {Funnel plots for comparing institutional performance},
	volume = {24},
	issn = {0277-6715 (Print) 0277-6715 (Linking)},
	doi = {10/fq7z8t},
	abstract = {'Funnel plots' are recommended as a graphical aid for institutional comparisons, in which an estimate of an underlying quantity is plotted against an interpretable measure of its precision. 'Control limits' form a funnel around the target outcome, in a close analogy to standard Shewhart control charts. Examples are given for comparing proportions and changes in rates, assessing association between outcome and volume of cases, and dealing with over-dispersion due to unmeasured risk factors. We conclude that funnel plots are flexible, attractively simple, and avoid spurious ranking of institutions into 'league tables'.},
	language = {english},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Spiegelhalter, D. J.},
	month = apr,
	year = {2005},
	keywords = {*Biometry, Adolescent, Cardiac Surgical Procedures/mortality, Coronary Artery Bypass/mortality/statistics \& numerical data, Cross-Sectional Studies, England/epidemiology, Female, Health Care, Humans, Infant, Odds Ratio, Outcome Assessment (Health Care), Pregnancy, Pregnancy in Adolescence/prevention \& control/statistics \& numerical data, Quality Assurance, Risk Factors},
	pages = {1185--202},
}

@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@article{spiegelhalterStatisticalMethodsHealthcare2012,
	title = {Statistical methods for healthcare regulation: {Rating}, screening and surveillance},
	volume = {175},
	issn = {09641998},
	doi = {10/dqpqpk},
	abstract = {Summary. Current demand for accountability and efficiency of healthcare organizations, combined with the greater availability of routine data on clinical care and outcomes, has led to an increased focus on statistical methods in healthcare regulation. We consider three different regulatory functions in which statistical analysis plays a vital role: rating organizations, deciding whom to inspect and continuous surveillance for arising problems. A common approach to data standardization based on (possibly overdispersed) Z-scores is proposed, although specific tools are used for assessing performance against a target, combining indicators when screening for inspection, and continuous monitoring using risk-adjusted sequential testing procedures. We pay particular attention to the problem of simultaneously monitoring over 200000 indicators for excess mortality, both with respect to the statistical issues surrounding massive multiplicity, and the organizational aspects of dealing with such a complex but high profile process.},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Spiegelhalter, David and Sherlaw-Johnson, Christopher and Bardsley, Martin and Blunt, Ian and Wood, Christopher and Grigg, Olivia},
	year = {2012},
	keywords = {Funnel plot, League table, Overdispersion, Risk-adjusted cumulative sum, Risk-based inspection, Statistical process control},
	pages = {1--47},
}

@article{steinerMonitoringSurgicalPerformance2000,
	title = {Monitoring surgical performance using risk-adjusted cumulative sum charts},
	volume = {1},
	issn = {1465-4644},
	doi = {10/bstchx},
	abstract = {The cumulative sum (CUSUM) procedure is a graphical method that is widely used for quality monitoring in industrial settings. More recently it has been used to monitor surgical outcomes whereby it `signals' if sufficient evidence has accumulated that there has been a change in the surgical failure rate. A limitation of the standard CUSUM procedure in this context is that since it is simply based on the observed surgical outcomes, it may signal as a result of changes in the referral pattern, such as an increased proportion of high-risk patients, rather than due to a change in the actual surgical performance. We describe a new CUSUM procedure that adjusts for each patient's pre-operative risk of surgical failure through the use of a likelihood-based scoring method. The procedure is therefore ideally suited for settings where there is a variable mix of patients over time.*To whom correspondence should be addressed},
	number = {4},
	journal = {Biostatistics (Oxford, England)},
	author = {Steiner, Stefan H. and Cook, Richard J. and Farewell, Vern T. and Treasure, Tom},
	month = dec,
	year = {2000},
	pages = {441--452},
}

@article{ulmSimpleMethodCalculate1990,
	title = {Simple {Method} to {Calculate} the {Confidence} {Interval} of a {Standardized} {Mortality} {Ratio} ({SMR})},
	volume = {131},
	issn = {0002-9262},
	doi = {10/gfz5xz},
	abstract = {In analyzing standardized mortality ratios (SMRs), it is of interest to calculate a confidence interval for the true SMR. The exact limits of a specific interval can be obtained by means of the Poisson distribution either within an iterative procedure or by one of the tables. The limits can be approximated in using one of various shortcut methods. In this paper, a method is described for calculating the exact limits in a simple and easy way. The method is based on the link between the x2 distribution and the Poisson distribution. Only a table of the x2 distribution is necessary.},
	number = {2},
	journal = {American Journal of Epidemiology},
	author = {Ulm, K.},
	year = {1990},
	pages = {373--375},
}

@article{woodallDesignCUSUMQuality1986,
	title = {The {Design} of {CUSUM} {Quality} {Control} {Charts}},
	volume = {18},
	issn = {0022-4065},
	doi = {10/gjzktw},
	abstract = {Recent methods of designing CUSUM quality control charts are reviewed. It is shown that the statistical performance of control procedures obtained using economic models is often unsatisfactory.},
	number = {2},
	journal = {Journal of Quality Technology},
	author = {Woodall, William H.},
	month = apr,
	year = {1986},
	pages = {99--102},
}

@article{woodallInertialPropertiesQuality2005,
	title = {The {Inertial} {Properties} of {Quality} {Control} {Charts}},
	volume = {47},
	issn = {00401706},
	doi = {10/frj5mg},
	abstract = {[Many types of control charts have an ability to detect process changes that can weaken over time depending on the past data observed. This is often referred to as the "inertia problem." We propose a new measure of inertia, the signal resistance, to be the largest standardized deviation from target not leading to an immediate out-of-control signal. We calculate the signal resistance values for several types of univariate and multivariate charts. Our conclusions support the recommendation that Shewhart limits should be used with exponentially weighted moving average charts, especially when the smoothing parameter is small.]},
	number = {4},
	journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
	author = {Woodall, William H. and Mahmoud, Mahmoud A.},
	year = {2005},
	pages = {425--436},
}

@article{woodallUseControlCharts2006,
	title = {The {Use} of {Control} {Charts} in {Health}-{Care} and {Public}-{Health} {Surveillance}},
	volume = {38},
	issn = {0022-4065},
	doi = {10/gjzktt},
	abstract = {There are many applications of control charts in health-care monitoring and in public-health surveillance. We introduce these applications to industrial practitioners and discuss some of the ideas that arise that may be applicable in industrial monitoring. The advantages and disadvantages of the charting methods proposed in the health-care and public-health areas are considered. Some additional contributions in the industrial statistical process control literature relevant to this area are given. There are many application and research opportunities available in the use of control charts for health-related monitoring.},
	number = {2},
	journal = {Journal of Quality Technology},
	author = {Woodall, William H.},
	month = apr,
	year = {2006},
	pages = {89--104},
}

@article{aderySimplifiedMonteCarlo1968,
	title = {A {Simplified} {Monte} {Carlo} {Significance} {Test} {Procedure}},
	volume = {30},
	issn = {00359246},
	doi = {10/gfvvc3},
	abstract = {[The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given α = 1/n, n a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is N(γ. 1). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.]},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Adery, C. A. Hope},
	year = {1968},
	pages = {582--598},
}

@unpublished{bolkerDealingQuasiModels2017,
	address = {CRAN},
	type = {Tutorial},
	title = {Dealing with quasi- models in {R}},
	author = {Bolker, Benjamin M.},
	month = oct,
	year = {2017},
}

@incollection{cameronGeneralizedCountRegression2013,
	address = {Cambridge},
	edition = {2},
	series = {Econometric {Society} {Monographs}},
	title = {Generalized {Count} {Regression}},
	isbn = {978-1-107-01416-9},
	abstract = {INTRODUCTIONThe most commonly used models for count regression, Poisson and negative binomial, were presented in Chapter 3. In this chapter we introduce richer models for count regression using cross-section data. For some of these models the conditional mean retains the exponential functional form. Then the Poisson QMLE and NB2 ML estimators remain consistent, although they may be inefficient and may not be suitable for predicting probabilities, rather than the conditional mean. For many of these models, however, the Poisson and NB2 estimators are inconsistent. Then alternative methods are used, ones that generally rely heavily on parametric assumptions.One reason for the failure of the Poisson regression is that the Poisson process has unobserved heterogeneity that contributes additional randomness. This leads to mixture models, the negative binomial being only one example. A second reason is the failure of the Poisson process assumption and its replacement by a more general stochastic process.Some common departures from the standard Poisson regression are as follows.Failure of the mean-equals-variance restriction: Frequently the conditional variance of data exceeds the conditional mean, which is usually referred to as extra-Poisson variation or overdispersion relative to the Poisson model. Overdispersion may result from neglected or unobserved heterogeneity that is inadequately captured by the covariates in the conditional mean function. It is common to allow for random variation in the Poisson conditional mean by introducing a multiplicative error term. This leads to families of mixed Poisson models.[…]},
	booktitle = {Regression {Analysis} of {Count} {Data}},
	publisher = {Cambridge University Press},
	author = {Cameron, A. C. and Trivedi, P. K.},
	editor = {Cameron, Colin A. and Trivedi, Pravin K.},
	year = {2013},
	pages = {111--176},
}

@article{efronBootstrapMethodsAnother1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	issn = {0090-5364},
	doi = {10/dj84pt},
	abstract = {We discuss the following problem: given a random sample \${\textbackslash}mathbf\{X\} = (X\_1, X\_2, {\textbackslash}cdots, X\_n)\$ from an unknown probability distribution \$F\$, estimate the sampling distribution of some prespecified random variable \$R({\textbackslash}mathbf\{X\}, F)\$, on the basis of the observed data \${\textbackslash}mathbf\{x\}\$. (Standard jackknife theory gives an approximate mean and variance in the case \$R({\textbackslash}mathbf\{X\}, F) = {\textbackslash}theta({\textbackslash}hat\{F\}) - {\textbackslash}theta(F), {\textbackslash}theta\$ some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
	language = {english},
	number = {1},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	keywords = {bootstrap, discriminant analysis, error rate estimation, Jackknife, nonlinear regression, nonparametric variance estimation, resampling, subsample values},
	pages = {1--26},
}

@article{famoyeRestrictedGeneralizedPoisson1993,
	title = {Restricted generalized poisson regression model},
	volume = {22},
	issn = {0361-0926},
	doi = {10/dkr9nc},
	abstract = {The family of generalized Poisson distribution has been found useful in describing over-dispersed and under-dispersed count data. We propose the use of restricted generalized Poisson regression model to predict a response variable affected by one or more explanatory variables. Approximate tests for the adequacy of the model and the estimation of the parameters are considered. Restricted generalized Poisson regression model has been applied to an observed data set.},
	number = {5},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Famoye, Felix},
	month = jan,
	year = {1993},
	pages = {1335--1354},
}

@article{nelderGeneralizedLinearModels1972,
	title = {Generalized {Linear} {Models}},
	volume = {135},
	issn = {00359238},
	doi = {10/dhq253},
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	number = {3},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Nelder, J. A. and Wedderburn, R. W. M.},
	year = {1972},
	pages = {370--384},
}

@article{poortemaModellingOverdispersionCounts1999,
	title = {On modelling overdispersion of counts},
	volume = {53},
	issn = {0039-0402},
	doi = {10/fxf4p2},
	abstract = {For counts it often occurs that the observed variance exceeds the nominal variance of the claimed binomial, multinomial or Poisson distributions. We study how models can be extended to cope with this phenomenon: a survey of literature is given. We focus on modelling, not on estimation or testing statistical hypotheses. The attention is restricted to independent observations.},
	number = {1},
	journal = {Statistica Neerlandica},
	author = {Poortema, K.},
	month = mar,
	year = {1999},
	keywords = {extra-binomial variation, extra-multinomial variation, extra-Poisson variation, models},
	pages = {5--20},
}

@article{sellersFlexibleRegressionModel2010,
	title = {A flexible regression model for count data},
	volume = {4},
	issn = {1932-6157},
	doi = {10/cn6mjv},
	abstract = {Poisson regression is a popular tool for modeling count data and is applied in a vast array of applications from the social to the physical sciences and beyond. Real data, however, are often over- or under-dispersed and, thus, not conducive to Poisson regression. We propose a regression model based on the Conway-Maxwell-Poisson (COM-Poisson) distribution to address this problem. The COM-Poisson regression generalizes the well-known Poisson and logistic regression models, and is suitable for fitting count data with a wide range of dispersion levels. With a GLM approach that takes advantage of exponential family properties, we discuss model estimation, inference, diagnostics, and interpretation, and present a test for determining the need for a COM-Poisson regression over a standard Poisson regression. We compare the COM-Poisson to several alternatives and illustrate its advantages and usefulness using three data sets with varying dispersion.},
	language = {english},
	number = {2},
	journal = {Annals of Applied Statistics},
	author = {Sellers, Kimberly F. and Shmueli, Galit},
	month = jun,
	year = {2010},
	keywords = {Conway-Maxwell-Poisson (COM-Poisson) distribution, dispersion, generalized linear models (GLM), generalized Poisson},
	pages = {943--961},
}


@article{shmueliExplainPredict2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237},
	doi = {10/dvwwrp},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	language = {english},
	number = {3},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	pages = {289--310},
}

@article{skondralLatentVariableModelling2007,
	title = {Latent {Variable} {Modelling}: {A} {Survey}*},
	volume = {34},
	doi = {10/bjdkgh},
	abstract = {Abstract. Latent variable modelling has gradually become an integral part of mainstream statistics and is currently used for a multitude of applications in different subject areas. Examples of `traditional' latent variable models include latent class models, item–response models, common factor models, structural equation models, mixed or random effects models and covariate measurement error models. Although latent variables have widely different interpretations in different settings, the models have a very similar mathematical structure. This has been the impetus for the formulation of general modelling frameworks which accommodate a wide range of models. Recent developments include multilevel structural equation models with both continuous and discrete latent variables, multiprocess models and nonlinear latent variable models.},
	number = {4},
	journal = {Scandinavian Journal of Statistics},
	author = {Skondral, Anders and Rabe-Hesketh, Sophia},
	year = {2007},
	pages = {712--745},
}

@article{spiegelhalterFunnelPlotsComparing2005,
	title = {Funnel plots for comparing institutional performance},
	volume = {24},
	issn = {0277-6715 (Print) 0277-6715 (Linking)},
	doi = {10/fq7z8t},
	abstract = {'Funnel plots' are recommended as a graphical aid for institutional comparisons, in which an estimate of an underlying quantity is plotted against an interpretable measure of its precision. 'Control limits' form a funnel around the target outcome, in a close analogy to standard Shewhart control charts. Examples are given for comparing proportions and changes in rates, assessing association between outcome and volume of cases, and dealing with over-dispersion due to unmeasured risk factors. We conclude that funnel plots are flexible, attractively simple, and avoid spurious ranking of institutions into 'league tables'.},
	language = {english},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Spiegelhalter, D. J.},
	month = apr,
	year = {2005},
	keywords = {*Biometry, Adolescent, Cardiac Surgical Procedures/mortality, Coronary Artery Bypass/mortality/statistics \& numerical data, Cross-Sectional Studies, England/epidemiology, Female, Health Care, Humans, Infant, Odds Ratio, Outcome Assessment (Health Care), Pregnancy, Pregnancy in Adolescence/prevention \& control/statistics \& numerical data, Quality Assurance, Risk Factors},
	pages = {1185--202},
}

@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@book{venablesModernAppliedStatistics2013,
	title = {Modern {Applied} {Statistics} with {S}-{Plus}},
	isbn = {978-1-4899-2820-7},
	publisher = {Springer New York},
	author = {Venables, W. N. and Ripley, B. D.},
	year = {2013},
}

@article{wedderburnQuasiLikelihoodFunctionsGeneralized1974,
	title = {Quasi-{Likelihood} {Functions}, {Generalized} {Linear} {Models}, and the {Gauss}-{Newton} {Method}},
	volume = {61},
	abstract = {To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specify a relation between the mean and variance of the observations and the quasi-likelihood can then be used for estimation. For a one-parameter exponential family the log likelihood is the same as the quasi-likelihood and it follows that assuming a one-parameter exponential family is the weakest sort of distributional assumption that can be made. The Gauss-Newton method for calculating nonlinear least squares estimates generalizes easily to deal with maximum quasi-likelihood estimates, and a rearrangement of this produces a generalization of the method described by Nelder \& Wedderburn (1972).},
	number = {3},
	journal = {Biometrika},
	author = {Wedderburn, R. W. M.},
	year = {1974},
	pages = {439--447},
	
}

@phdthesis{maineyStatisticalMethodsNHS2020,
	address = {London},
	title = {Statistical methods for {NHS} incident reporting data},
	abstract = {The National Reporting and Learning System (NRLS) is the English and Welsh NHS' national repository of incident reports from healthcare. It aims to capture details of incident reports, at national level, and facilitate clinical review and learning to improve patient safety. These incident reports range from minor `near-misses' to critical incidents that may lead to severe harm or death. NRLS data are currently reported as crude counts and proportions, but their major use is clinical review of the free-text descriptions of incidents. There are few well-developed quantitative analysis approaches for NRLS, and this thesis investigates these methods. A literature review revealed a wealth of clinical detail, but also systematic constraints of NRLS' structure, including non-mandatory reporting, missing data and misclassification. Summary statistics for reports from 2010/11 – 2016/17 supported this and suggest NRLS was not suitable for statistical modelling in isolation. Modelling methods were advanced by creating a hybrid dataset using other sources of hospital casemix data from Hospital Episode Statistics (HES). A theoretical model was established, based on `exposure' variables (using casemix proxies), and `culture' as a random-effect. The initial modelling approach examined Poisson regression, mixture and multilevel models. Overdispersion was significant, generated mainly by clustering and aggregation in the hybrid dataset, but models were chosen to reflect these structures. Further modelling approaches were examined, using Generalized Additive Models to smooth predictor variables, regression tree-based models including Random Forests, and Artificial Neural Networks. Models were also extended to examine a subset of death and severe harm incidents, exploring how sparse counts affect models. Text mining techniques were examined for analysis of incident descriptions and showed how term frequency might be used. Terms were used to generate latent topics models used, in-turn, to predict the harm level of incidents. Model outputs were used to create a `Standardised Incident Reporting Ratio' (SIRR) and cast this in the mould of current regulatory frameworks, using process control techniques such as funnel plots and cusum charts. A prototype online reporting tool was developed to allow NHS organisations to examine their SIRRs, provide supporting analyses, and link data points back to individual incident reports.},
	language = {english},
	school = {UCL},
	author = {Mainey, Christopher},
	year = {2020},
	URL = {https://discovery.ucl.ac.uk/id/eprint/10094736/}
}

@article{chouldechovaGeneralizedAdditiveModel2015,
	title = {Generalized {Additive} {Model} {Selection}},
	volume = {1506.03850},
	journal = {arXiv.org},
	author = {Chouldechova, Alexandra and Hastie, Trevor},
	month = jun,
	year = {2015},
	keywords = {⛔ No DOI found},
	pages = {23},
}

@article{hastieGeneralizedAdditiveModels1986,
	title = {Generalized {Additive} {Models}},
	volume = {1},
	issn = {08834237},
	abstract = {Likelihood-based regression models such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariates X₁, X₂, \&\#x22ef;, Xₚ. We introduce the class of generalized additive models which replaces the linear form \&\#x2211; \&\#x3b2;$_{\textrm{jXj}}$ by a sum of smooth functions \&\#x2211; s$_{\textrm{j}}$(X$_{\textrm{j}}$). The s$_{\textrm{j}}$(\&\#xb7;)'s are unspecified functions that are estimated using a scatterplot smoother, in an iterative procedure we call the local scoring algorithm. The technique is applicable to any likelihood-based regression model: the class of generalized linear models contains many of these. In this class the linear predictor \&\#x3b7; = \&\#x3a3; \&\#x3b2;$_{\textrm{jXj}}$ is replaced by the additive predictor \&\#x3a3; s$_{\textrm{j}}$(X$_{\textrm{j}}$); hence, the name generalized additive models. We illustrate the technique with binary response and survival data. In both cases, the method proves to be useful in uncovering nonlinear covariate effects. It has the advantage of being completely automatic, i.e., no "detective work" is needed on the part of the statistician. As a theoretical underpinning, the technique is viewed as an empirical method of maximizing the expected log likelihood, or equivalently, of minimizing the Kullback-Leibler distance to the true model.},
	number = {3},
	journal = {Statistical Science},
	author = {Hastie, Trevor and Tibshirani, Robert},
	year = {1986},
	keywords = {❓ Multiple DOI},
	pages = {297--310},
}

@article{reinschSmoothingSplineFunctions1967,
	title = {Smoothing by spline functions},
	volume = {10},
	issn = {0029-599X},
	doi = {10/d2ftmg},
	number = {3},
	journal = {Numerische mathematik},
	author = {Reinsch, Christian H.},
	year = {1967},
	pages = {177--183},
}

@book{woodGeneralizedAdditiveModels2006,
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-1-58488-474-3},
	publisher = {Taylor \& Francis},
	author = {Wood, Simon N.},
	year = {2006},
}

@article{breimanBaggingPredictors1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	doi = {10/fm3fwj},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	number = {2},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	pages = {123--140},
}

@article{breimanRandomForests2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {08856125},
	doi = {10/d8zjwq},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
}

@article{elithWorkingGuideBoosted2008,
	title = {A working guide to boosted regression trees},
	volume = {77},
	issn = {0021-8790},
	doi = {10/fn6m6v},
	abstract = {1. Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2. This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3. Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4. The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.},
	language = {english},
	number = {4},
	journal = {Journal of Animal Ecology},
	author = {Elith, J. and Leathwick, J. R. and Hastie, T.},
	month = jul,
	year = {2008},
	keywords = {*Models, *Statistics as Topic, Anguilla/growth \& development/*physiology, Animals, Biological, Decision Trees, Ecology/*statistics \& numerical data, Models, Regression Analysis, Statistical},
	pages = {802--13},
}

@article{friedmanStochasticGradientBoosting2002,
	title = {Stochastic {Gradient} {Boosting}},
	volume = {38},
	doi = {10/fxb956},
	abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current “pseudo”-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	year = {2002},
	pages = {367--378},
}

@book{goodfellowDeepLearning2018,
	address = {Cambridge, Massachutsetts \& London, England},
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Deep {Learning}},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	editor = {Back, Francis},
	year = {2018},
}

@book{hastieElementsStatisticalLearning2009,
	address = {New York, NETHERLANDS},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning} : {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-84858-7},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	keywords = {Data mining., Electronic books. – local, Supervised learning (Machine learning)},
}

@article{leapeNatureAdverseEvents1991,
	title = {The {Nature} of {Adverse} {Events} in {Hospitalized} {Patients}},
	volume = {324},
	doi = {10/fhvzbs},
	number = {6},
	journal = {New England Journal of Medicine},
	author = {Leape, Lucian L. and Brennan, Troyen A. and Laird, Nan and Lawthers, Ann G. and Localio, A. Russell and Barnes, Benjamin A. and Hebert, Liesi and Newhouse, Joseph P. and Weiler, Paul C. and Hiatt, Howard},
	year = {1991},
	pages = {377--384},
}

@article{donaldsonOrganisationMemory2002,
	title = {An organisation with a memory},
	volume = {2},
	issn = {1470-2118 (Print) 1470-2118},
	doi = {10/gfz5w4},
	abstract = {Patient safety has been an under-recognised and under-researched concept until recently. It is now high on the healthcare quality agenda in many countries of the world including the UK. The recognition that human error is inevitable in a highly complex and technical field like medicine is a first step in promoting greater awareness of the importance of systems failure in the causation of accidents. Plane crashes are not usually caused by pilot error per se but by an amalgam of technical, environmental, organisational, social and communication factors which predispose to human error or worsen its consequences. In healthcare, the systematic investigation of error in the administration of medication will often reveal similarly complex causation. Experience and research from other sectors, in particular the airline industry, show that the impact of human error can be reduced if the necessary work is put in to detect and then remove weaknesses and vulnerabilties in the system. The NHS is putting in place a comprehensive programme to learn more effectively from adverse events and near misses. This aims to reduce the burden of the estimated 850,000 adverse events which occur in hospitals each year as well as targeting high risk areas such as medication error.},
	language = {english},
	number = {5},
	journal = {Clinical Medicine (London, England)},
	author = {Donaldson, L.},
	month = sep,
	year = {2002},
	keywords = {Awareness, Drug Labeling, Drug Packaging, Great Britain, Humans, Medical Errors/*prevention \& control/statistics \& numerical data, Risk Management/*methods, Safety Management/*methods},
	pages = {452--7},
}

@incollection{kohnErrHumanBuilding2000,
	address = {Washington (DC)},
	title = {To {Err} is {Human}: {Building} a {Safer} {Health} {System}},
	abstract = {Experts estimate that as many as 98,000 people die in any given year from medical errors that occur in hospitals. That's more than die from motor vehicle accidents, breast cancer, or AIDS–three causes that receive far more public attention. Indeed, more people die annually from medication errors than from workplace injuries. Add the financial cost to the human tragedy, and medical error easily rises to the top ranks of urgent, widespread public problems. To Err Is Human breaks the silence that has surrounded medical errors and their consequence–but not by pointing fingers at caring health care professionals who make honest mistakes. After all, to err is human. Instead, this book sets forth a national agenda–with state and local implications–for reducing medical errors and improving patient safety through the design of a safer health system. This volume reveals the often startling statistics of medical error and the disparity between the incidence of error and public perception of it, given many patients' expectations that the medical profession always performs perfectly. A careful examination is made of how the surrounding forces of legislation, regulation, and market activity influence the quality of care provided by health care organizations and then looks at their handling of medical mistakes. Using a detailed case study, the book reviews the current understanding of why these mistakes happen. A key theme is that legitimate liability concerns discourage reporting of errors–which begs the question, "How can we learn from our mistakes?" Balancing regulatory versus market-based initiatives and public versus private efforts, the Institute of Medicine presents wide-ranging recommendations for improving patient safety, in the areas of leadership, improved data collection and analysis, and development of effective systems at the level of direct patient care. To Err Is Human asserts that the problem is not bad people in health care–it is that good people are working in bad systems that need to be made safer. Comprehensive and straightforward, this book offers a clear prescription for raising the level of patient safety in American health care. It also explains how patients themselves can influence the quality of care that they receive once they check into the hospital. This book will be vitally important to federal, state, and local health policy makers and regulators, health professional licensing officials, hospital administrators, medical educators and students, health caregivers, health journalists, patient advocates–as well as patients themselves. First in a series of publications from the Quality of Health Care in America, a project initiated by the Institute of Medicine},
	language = {english},
	booktitle = {To {Err} is {Human}: {Building} a {Safer} {Health} {System}},
	author = {Kohn, L. T. and Corrigan, J. M. and Donaldson, M. S.},
	editor = {Kohn, L. T. and Corrigan, J. M. and Donaldson, M. S.},
	year = {2000},
}

@book{poissonRecherchesProbabiliteJugements1837,
	title = {Recherches sur la probabilit\'{e} des jugements en mati\'{e}re criminelle et en mati\`{e}re civile: pr\'{e}c\'{e}d\'{e}es des r\`{e}gles g\'{e}n\'{e}rales du calcul des probabilit\'{e}s},
	publisher = {Bachelier},
	author = {Poisson, S. D.},
	year = {1837},
}


@article{spiegelhalterHandlingOverdispersionPerformance2005,
	title = {Handling over-dispersion of performance indicators},
	volume = {14},
	doi = {10/cdmxpn},
	abstract = {Objectives: A problem can arise when a performance indicator shows substantially more variability than would be expected by chance alone, since ignoring such “over-dispersion” could lead to a large number of institutions being inappropriately classified as “abnormal”. A number of options for handling this phenomenon are investigated, ranging from improved risk stratification to fitting a statistical model that robustly estimates the degree of over-dispersion. Design: Retrospective analysis of publicly available data on survival following coronary artery bypass grafts, emergency readmission rates, and teenage pregnancies. Setting: NHS trusts in England. Results: Funnel plots clearly show the influence of the method chosen for dealing with over-dispersion on the “banding” a trust receives. Both multiplicative and additive approaches are feasible and give intuitively reasonable results, but the additive random effects formulation appears to have a stronger conceptual foundation. Conclusion: A random effects model may offer a reasonable solution. This method has now been adopted by the UK Healthcare Commission in their derivation of star ratings.},
	number = {5},
	journal = {Quality and Safety in Health Care},
	author = {Spiegelhalter, D. J.},
	year = {2005},
	pages = {347--351},
}

@article{bleiLatentDirichletAllocation2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	issn = {1532-4435},
	journal = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2003},
	keywords = {⛔ No DOI found},
	pages = {993--1022},
}

@article{bottlePredictingFalseAlarm2011,
	title = {Predicting the false alarm rate in multi-institution mortality monitoring},
	volume = {62},
	issn = {01605682, 14769360},
	doi = {10/cbr4rq},
	abstract = {Statistical process control is increasingly used by single hospitals or centres to monitor their performance, but national monitoring across multiple centres, measures and groups incurs higher false alarm rates unless the method is modified. We consider setting the threshold for cumulative sum charts to produce the desired false alarm rate, taking into account the centre volume and expected outcome rate. We used simulation to estimate the false alarm and successful detection rates for a variety of chart thresholds. We thereby calculated the 'cost' of a higher threshold compared with one set to give a false alarm rate of 5\% for three clinical groups of common interest. The false alarm rate often showed non-linear relations with the threshold, volume and expected mortality rate but an equation was found with good approximation to the simulated values. The relation between these factors and the 'cost' of a higher threshold was not straightforward. The 'cost' (difference in number of deaths) incurred by raising the chart threshold provides an intuitive measure and is applicable to other settings.},
	number = {9},
	journal = {The Journal of the Operational Research Society},
	author = {Bottle, A. and Aylin, P.},
	year = {2011},
	pages = {1711--1718},
}

@article{jarmanExplainingDifferencesEnglish1999,
	title = {Explaining differences in {English} hospital death rates using routinely collected data},
	volume = {318},
	issn = {0959-8138 1468-5833},
	doi = {10/fkkfm9},
	abstract = {Objectives: To ascertain hospital inpatient mortality in England and to determine which factors best explain variation in standardised hospital death ratios. Design: Weighted linear regression analysis of routinely collected data over four years, with hospital standardised mortality ratios as the dependent variable. Setting: England. Subjects:Eight million discharges from NHS hospitals when the primary diagnosis was one of the diagnoses accounting for 80\% of inpatient deaths. Main outcome measures: Hospital standardised mortality ratios and predictors of variations in these ratios. Results: The four year crude death rates varied across hospitals from 3.4\% to 13.6\% (average for England 8.5\%), and standardised hospital mortality ratios ranged from 53 to 137 (average for England 100). The percentage of cases that were emergency admissions (60\% of total hospital admissions) was the best predictor of this variation in mortality, with the ratio of hospital doctors to beds and general practitioners to head of population the next best predictors. When analyses were restricted to emergency admissions (which covered 93\% of all patient deaths analysed) number of doctors per bed was the best predictor. Conclusion: Analysis of hospital episode statistics reveals wide variation in standardised hospital mortality ratios in England. The percentage of total admissions classified as emergencies is the most powerful predictor of variation in mortality. The ratios of doctors to head of population served, both in hospital and in general practice, seem to be critical determinants of standardised hospital death rates; the higher these ratios, the lower the death rates in both cases.\%U http://www.bmj.com/content/bmj/318/7197/1515.full.pdf},
	number = {7197},
	journal = {BMJ},
	author = {Jarman, B. and Gault, S. and Alves, B. and Hider, A. and Dolan, S. and Cook, A. and Hurwitz, B. and Iezzoni, L. I.},
	year = {1999},
	pages = {1515--1520},
}

@article{campbellDevelopingSummaryHospital2012,
	title = {Developing a summary hospital mortality index: retrospective analysis in {English} hospitals over five years},
	volume = {344},
	doi = {10/gb3r9t},
	abstract = {Objectives To develop a transparent and reproducible measure for hospitals that can indicate when deaths in hospital or within 30 days of discharge are high relative to other hospitals, given the characteristics of the patients in that hospital, and to investigate those factors that have the greatest effect in changing the rank of a hospital, whether interactions exist between those factors, and the stability of the measure over time.Design Retrospective cross sectional study of admissions to English hospitals.Setting Hospital episode statistics for England from 1 April 2005 to 30 September 2010, with linked mortality data from the Office for National Statistics.Participants 36.5 million completed hospital admissions in 146 general and 72 specialist trusts.Main outcome measures Deaths within hospital or within 30 days of discharge from hospital.Results The predictors that were used in the final model comprised admission diagnosis, age, sex, type of admission, and comorbidity. The percentage of people admitted who died in hospital or within 30 days of discharge was 4.2\% for males and 4.5\% for females. Emergency admissions comprised 75\% of all admissions and 5.5\% died, in contrast to 0.8\% who died after an elective admission. The percentage who died with a Charlson comorbidity score of 0 was 2\% in contrast with 15\% who died with a score greater than 5. Given these variables, the relative standardised mortality rates of the hospitals were not noticeably changed by adjusting for the area level deprivation and number of previous emergency visits to hospital. There was little evidence that including interaction terms changed the relative values by any great amount. Using these predictors the summary hospital mortality index (SHMI) was derived. For 2007/8 the model had a C statistic of 0.911 and accounted for 81\% of the variability of between hospital mortality. A random effects funnel plot was used to identify outlying hospitals. The outliers from the SHMI over the period 2005-10 have previously been identified using other mortality indicators. Conclusion The SHMI is a relatively simple tool that can be used in conjunction with other information to identify hospitals that may need further investigation.},
	journal = {BMJ},
	author = {Campbell, Michael J. and Jacques, Richard M. and Fotheringham, James and Maheswaran, Ravi and Nicholl, Jon},
	year = {2012},
	pages = {e1001},
}

@misc{keoghKeoghReviewHospital2013,
  title = {Keogh Review on Hospital Deaths Published - {{NHSUK}}},
  author = {Keogh, B.},
  year = {2013},
  abstract = {The findings of a review into the quality of care and treatment provided by 14 hospital trusts in England has prompted widespread coverage in the press, with BBC News reporting that\ldots},
  keywords = {National Health Service (NHS); Bereavement support services; Death and dying; Emergency and urgent care; End-of-life issues; Hospitals; NHS structure},
}


@article{girlingCasemixAdjustedHospital2012,
  title = {Case-Mix Adjusted Hospital Mortality Is a Poor Proxy for Preventable Mortality: A Modelling Study},
  author = {Girling, Alan J. and Hofer, Timothy P. and Wu, Jianhua and Chilton, Peter J. and Nicholl, Jonathan P. and Mohammed, Mohammed A. and Lilford, Richard J.},
  year = {2012},
  journal = {BMJ Quality \& Safety},
  volume = {21},
  number = {12},
  pages = {1052--1056},
  doi = {10/f4fr3b},
  abstract = {Risk-adjustment schemes are used to monitor hospital performance, on the assumption that excess mortality not explained by case mix is largely attributable to suboptimal care. We have developed a model to estimate the proportion of the variation in standardised mortality ratios (SMRs) that can be accounted for by variation in preventable mortality. The model was populated with values from the literature to estimate a predictive value of the SMR in this context\textemdash specifically the proportion of those hospitals with SMRs among the highest 2.5\% that fall among the worst 2.5\% for preventable mortality. The extent to which SMRs reflect preventable mortality rates is highly sensitive to the proportion of deaths that are preventable. If 6\% of hospital deaths are preventable (as suggested by the literature), the predictive value of the SMR can be no greater than 9\%. This value could rise to 30\%, if 15\% of deaths are preventable. The model offers a `reality check' for case mix adjustment schemes designed to isolate the preventable component of any outcome rate.\%U http://qualitysafety.bmj.com/content/qhc/21/12/1052.full.pdf},
}


@article{hoganAvoidabilityHospitalDeaths2015,
  title = {Avoidability of Hospital Deaths and Association with Hospital-Wide Mortality Ratios: Retrospective Case Record Review and Regression Analysis},
  author = {Hogan, Helen and Zipfel, Rebecca and Neuburger, Jenny and Hutchings, Andrew and Darzi, Ara and Black, Nick},
  year = {2015},
  month = jul,
  journal = {BMJ},
  volume = {351},
  doi = {10/gb3swm},
  abstract = {Objectives To determine the proportion of avoidable deaths (due to acts of omission and commission) in acute hospital trusts in England and to determine the association with the trust's hospital-wide standardised mortality ratio assessed using the two commonly used methods - the hospital standardised mortality ratio (HSMR) and the summary hospital level mortality indicator (SHMI).Design Retrospective case record review of deaths.Setting 34 English acute hospital trusts (10 in 2009 and 24 in 2012/13) randomly selected from across the spectrum of HSMR.Main outcome measures Avoidable death, defined as those with at least a 50\% probability of avoidability in view of trained medical reviewers. Association of avoidable death proportion with the HSMR and the SHMI assessed using regression coefficients, to estimate the increase in avoidable death proportion for a one standard deviation increase in standardised mortality ratio.Participants 100 randomly selected hospital deaths from each trust.Results The proportion of avoidable deaths was 3.6\% (95\% confidence interval 3.0\% to 4.3\%). It was lower in 2012/13 (3.0\%, 2.4\% to 3.7\%) than in 2009 (5.2\%, 3.8\% to 6.6\%). This difference is subject to several factors, including reviewers' greater awareness in 2012/13 of orders not to resuscitate, patients being perceived as sicker on admission, minor differences in review form questions, and cultural changes that might have discouraged reviewers from criticising other clinicians. There was a small but statistically non-significant association between HSMR and the proportion of avoidable deaths (regression coefficient 0.3, 95\% confidence interval -0.2 to 0.7). The regression coefficient was similar for both time periods (0.1 and 0.3). This implies that a difference in HSMR of between 105 and 115 would be associated with an increase of only 0.3\% (95\% confidence interval -0.2\% to 0.7\%) in the proportion of avoidable deaths. A similar weak non-significant association was observed for SHMI (regression coefficient 0.3, 95\% confidence interval -0.3 to 1.0).Conclusions The small proportion of deaths judged to be avoidable means that any metric based on mortality is unlikely to reflect the quality of a hospital. The lack of association between the proportion of avoidable deaths and hospital-wide SMRs partly reflects methodological shortcomings in both metrics. Instead, reviews of individual deaths should focus on identifying ways of improving the quality of care, whereas the use of standardised mortality ratios should be restricted to assessing the quality of care for conditions with high case fatality for which good quality clinical data exist.},
}

@article{jarmanMonitoringChangesHospital2005,
  title = {Monitoring Changes in Hospital Standardised Mortality Ratios},
  author = {Jarman, Brian and Bottle, Alex and Aylin, Paul and Browne, Mike},
  year = {2005},
  month = feb,
  journal = {BMJ (Clinical research ed.)},
  volume = {330},
  number = {7487},
  pages = {329--329},
  publisher = {{BMJ Publishing Group Ltd.}},
  issn = {1756-1833},
  doi = {10.1136/bmj.330.7487.329},
  langid = {english},
  keywords = {England/epidemiology,Hospital Mortality/*trends,Humans,Risk Factors,Sentinel Surveillance,Survival Rate}
}

@article{wrightLearningDeathHospital2006,
  title = {Learning from Death: A Hospital Mortality Reduction Programme},
  author = {Wright, John and Dugdale, Bob and Hammond, Ian and Jarman, Brian and Neary, Maria and Newton, Duncan and Patterson, Chris and Russon, Lynne and Stanley, Philip and Stephens, Rose and Warren, Erica},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Society of Medicine},
  volume = {99},
  number = {6},
  pages = {303--308},
  publisher = {{The Royal Society of Medicine}},
  issn = {0141-0768},
  doi = {10.1258/jrsm.99.6.303},
  abstract = {PROBLEM: There are wide variations in hospital mortality. Much of this variation remains unexplained and may reflect quality of care. SETTING: A large acute hospital in an urban district in the North of England. DESIGN: Before and after evaluation of a hospital mortality reduction programme. STRATEGIES FOR CHANGE: Audit of hospital deaths to inform an evidence-based approach to identify processes of care to target for the hospital strategy. Establishment of a hospital mortality reduction group with senior leadership and support to ensure the alignment of the hospital departments to achieve a common goal. Robust measurement and regular feedback of hospital deaths using statistical process control charts and summaries of death certificates and routine hospital data. Whole system working across a health community to provide appropriate end of life care. Training and awareness in processes of high quality care such as clinical observation, medication safety and infection control. EFFECTS: Hospital standardized mortality ratios fell significantly in the 3 years following the start of the programme from 94.6 (95\% confidence interval 89.4, 99.9) in 2001 to 77.5 (95\% CI 73.1, 82.1) in 2005. This translates as 905 fewer hospital deaths than expected during the period 2002-2005. LESSONS LEARNT: Improving the safety of hospital care and reducing hospital deaths provides a clear and well supported goal from clinicians, managers and patients. Good leadership, good information, a quality improvement strategy based on good local evidence and a community-wide approach may be effective in improving the quality of processes of care sufficiently to reduce hospital mortality.},
  langid = {english},
  keywords = {*Hospital Mortality,England,Hospitals; Public/*standards,Humans,Medical Audit,Urban Health}
}

@article{skellamProbabilityDistributionDerived1948,
  title = {A {{Probability Distribution Derived}} from the {{Binomial Distribution}} by {{Regarding}} the {{Probability}} of {{Success}} as {{Variable Between}} the {{Sets}} of {{Trials}}},
  author = {Skellam, J. G.},
  year = {1948},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {10},
  number = {2},
  pages = {257--261},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {00359246}
}

@article{iezzoniRisksRiskAdjustment1997,
  title = {The Risks of Risk Adjustment},
  author = {Iezzoni, L. I.},
  year = {1997},
  month = nov,
  journal = {JAMA},
  volume = {278},
  number = {19},
  pages = {1600--7},
  issn = {0098-7484 (Print) 0098-7484 (Linking)},
  doi = {10/c2trv4},
  abstract = {CONTEXT: Risk adjustment is essential before comparing patient outcomes across hospitals. Hospital report cards around the country use different risk adjustment methods. OBJECTIVES: To examine the history and current practices of risk adjusting hospital death rates and consider the implications for using risk-adjusted mortality comparisons to assess quality. DATA SOURCES AND STUDY SELECTION: This article examines severity measures used in states and regions to produce comparisons of risk-adjusted hospital death rates. Detailed results are presented from a study comparing current commercial severity measures using a single database. It included adults admitted for acute myocardial infarction (n=11880), coronary artery bypass graft surgery (n=7765), pneumonia (n=18016), and stroke (n=9407). Logistic regressions within each condition predicted in-hospital death using severity scores. Odds ratios for in-hospital death were compared across pairs of severity measures. For each hospital, z scores compared actual and expected death rates. RESULTS: The severity measure called Disease Staging had the highest c statistic (which measures how well a severity measure discriminates between patients who lived and those who died) for acute myocardial infarction, 0.86; the measure called All Patient Refined Diagnosis Related Groups had the highest for coronary artery bypass graft surgery, 0.83; and the measure, MedisGroups, had the highest for pneumonia, 0.85 and stroke, 0.87. Different severity measures predicted different probabilities of death for many patients. Severity measures frequently disagreed about which hospitals had particularly low or high z scores. Agreement in identifying low- and high-mortality hospitals between severity-adjusted and unadjusted death rates was often better than agreement between severity measures. CONCLUSIONS: Severity does not explain differences in death rates across hospitals. Different severity measures frequently produce different impressions about relative hospital performance. Severity-adjusted mortality rates alone are unlikely to isolate quality differences across hospitals.},
  langid = {english},

}

@article{deeksEvaluatingNonrandomisedIntervention2003,
  title = {Evaluating Non-Randomised Intervention Studies.},
  author = {Deeks, Jonathan J and Dinnes, Jac and D'Amico, Roberto and Sowden, Amanda J and Sakarovitch, Charlotte and Song, Fujian and Petticrew, Mark and Altman, DG},
  year = {2003},
  journal = {Health technology assessment (Winchester, England)},
  volume = {7},
  number = {27},
  pages = {iii-173},
  publisher = {{NIHR Journals Library}},
  issn = {1366-5278}
}